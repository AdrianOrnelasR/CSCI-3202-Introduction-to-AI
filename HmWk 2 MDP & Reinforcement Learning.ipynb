{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# CSCI 3202, Fall 2022\n",
    "# Homework 2: MDP & Reinforcement Learning\n",
    "# Due: Friday September 9, 2022 at 6:00 PM\n",
    "\n",
    "<br> \n",
    "\n",
    "### Your name:\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# added packages\n",
    "import heapq\n",
    "from matplotlib import colors\n",
    "import random\n",
    "import builtins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "Consider a **cube** state space defined by $0 \\le x, y, z \\le L$. Suppose you are piloting/programming a drone to learn how to land on a platform at the center of the $z=0$ surface (the bottom). Some assumptions:\n",
    "* In this discrete world, if the drone is at $(x,y,z)$ it means that the box is centered at $(x,y,z)$. There are boxes (states) centered at $(x,y,z)$ for all $0 \\le x,y,z \\le L$. Each state is a 1 unit cube. \n",
    "* In this world, $L$ is always an even value.\n",
    "* All of the states with $z=0$ are terminal states.\n",
    "* The state at the center of the bottom of the cubic state space is the landing pad. For example, when $L=4$, the landing pad is at $(x,y,z) = (2,2,0)$.\n",
    "* All terminal states ***except*** the landing pad have a reward of -1. The landing pad has a reward of +1.\n",
    "* All non-terminal states have a living reward of -0.01.\n",
    "* The drone takes up exactly 1 cubic unit, and begins in a random non-terminal state.\n",
    "* The available actions in non-terminal states include moving exactly 1 unit Up (+z), Down (-z), North (+y), South (-y), East (+x) or West (-x). In a terminal state, the training episode should end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "How many states would be in the discrete state space if $L=2$? Explain your reasoning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "`The state space is depending on the volume of the cube. In this case we would have a 27 cubic volume for the state space. My reason for that is because we will have a layer 0, 1, and 2. Which means out state space is more like a rubics cube in where out drone is 1 unit space.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "Write a class `MDPLanding` to represent the Markov decision process for this drone. Include methods for:\n",
    "1. `actions(state)`, which should return a list of all actions available from the given state\n",
    "2. `reward(state)`, which should return the reward for the given state\n",
    "3. `result(state, action)`, which should return the resulting state of doing the given action in the given state\n",
    "\n",
    "and attributes for:\n",
    "1. `states`, a list of all the states in the state space, where each state is represented as an $(x,y,z)$ tuple\n",
    "2. `terminal_states`, a dictionary where keys are the terminal state tuples and the values are the rewards associated with those terminal states\n",
    "3. `default_reward`, a scalar for the reward associated with non-terminal states\n",
    "4. `all_actions`, a list of all possible actions (Up, Down, North, South, East, West)\n",
    "5. `discount`, the discount factor (use $\\gamma = 0.999$ for this entire problem)\n",
    "\n",
    "How you feed arguments/information into the class constructor is up to you.\n",
    "\n",
    "Note that actions are *deterministic* here.  The drone does not need to include transition probabilities for outcomes of particular actions. What the drone does need to learn, however, is where the landing pad is, and how to get there from any initial state.\n",
    "\n",
    "Before moving on to Part C, we recommend that you test that your MDPLanding code is set up correctly. Write unit tests that display the actions for a given state, rewards, results, etc. This will help you identify errors in your implementation and save you a lot of debugging time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "\n",
    "class MDPLanding:\n",
    "    # attributes\n",
    "    def __init__(self, state_lst, dict_lst, r):\n",
    "        # a list of all states in the state space (x,y,z)\n",
    "        self.states = state_lst \n",
    "        # key = terminal state, value = reward of terminal state\n",
    "        self.terminal_states = dict_lst\n",
    "        # reward associated with non-terminal state\n",
    "        self.default_reward = r \n",
    "        # list of possible actions\n",
    "        # self.all_actions = ['Up', 'Down', 'North', 'South', 'East', 'West']\n",
    "        self.all_actions = [(0,0,1), (0,0,-1), (0,1,0), (0,-1,0), (1, 0, 0), (-1, 0, 0)]\n",
    "        # discount factor which stays consistance\n",
    "        self.discount = 0.999\n",
    "\n",
    "    # return a list of all action available from the given state \n",
    "    def actions(self, state):\n",
    "        if(state[2] == 0):\n",
    "            result_state = []\n",
    "            for x in self.all_actions:\n",
    "                if(x != (0,0,-1)):\n",
    "                    result_state.append(x)\n",
    "            return result_state\n",
    "        \n",
    "        elif(state[2] == 4):\n",
    "            result_state = []\n",
    "            for x in self.all_actions:\n",
    "                if(x != (0,0,1)):\n",
    "                    result_state.append(x)\n",
    "            return result_state\n",
    "\n",
    "        elif(state[1] == 4):\n",
    "            result_state = []\n",
    "            for x in self.all_actions:\n",
    "                if(x != (0,1,0)):\n",
    "                    result_state.append(x)\n",
    "            return result_state             \n",
    "\n",
    "        elif(state[1] == 0):\n",
    "            result_state = []\n",
    "            for x in self.all_actions:\n",
    "                if(x != (0,-1,0)):\n",
    "                    result_state.append(x)\n",
    "            return result_state             \n",
    "\n",
    "        elif(state[0] == 4):\n",
    "            result_state = []\n",
    "            for x in self.all_actions:\n",
    "                if(x != (1,0,0)):\n",
    "                    result_state.append(x)\n",
    "            return result_state            \n",
    "\n",
    "        elif(state[0] == 0):\n",
    "            result_state = []\n",
    "            for x in self.all_actions:\n",
    "                if(x != (-1,0,0)):\n",
    "                    result_state.append(x)\n",
    "            return result_state         \n",
    "\n",
    "        else:\n",
    "            return self.all_actions\n",
    "            \n",
    "    #  return the reward for the given state\n",
    "    def reward(self, state):\n",
    "        if(state in self.terminal_states):\n",
    "            return self.terminal_states[state] \n",
    "        else:\n",
    "            return self.default_reward\n",
    "            \n",
    "    # return the resulting state of doing the given action in the given state\n",
    "    def result(self, state, action):\n",
    "        res = tuple(map(lambda i, j: i + j, state, action))\n",
    "        # if statments are used to check if you would hit an unaccecable location by taking that actions\n",
    "        if(res[0] > 4 or res[0] < 0):\n",
    "            return state\n",
    "        elif(res[1] > 4 or res[1] < 0):\n",
    "            return state\n",
    "        elif(res[2] > 4 or res[2] < 0):\n",
    "            return state\n",
    "        else:\n",
    "            return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part C\n",
    "Write a function to implement **policy iteration** for this drone landing MDP. Create an MDP environment to represent the $L=4$ case.\n",
    "\n",
    "Use your function to find an optimal policy for your new MDP environment. Check (by printing to screen) that the policy for the following states are what you expect, and **comment on the results**:\n",
    "1. $(2,2,1)$\n",
    "1. $(0,2,1)$\n",
    "1. $(2,0,1)$\n",
    "\n",
    "The policy for each of these states is the action that the agent should take in that state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0, 0): -1.0, (0, 1, 0): -1.0, (0, 2, 0): -1.0, (0, 3, 0): -1.0, (0, 4, 0): -1.0, (1, 0, 0): -1.0, (1, 1, 0): -1.0, (1, 2, 0): -1.0, (1, 3, 0): -1.0, (1, 4, 0): -1.0, (2, 0, 0): -1.0, (2, 1, 0): -1.0, (2, 2, 0): 1.0, (2, 3, 0): -1.0, (2, 4, 0): -1.0, (3, 0, 0): -1.0, (3, 1, 0): -1.0, (3, 2, 0): -1.0, (3, 3, 0): -1.0, (3, 4, 0): -1.0, (4, 0, 0): -1.0, (4, 1, 0): -1.0, (4, 2, 0): -1.0, (4, 3, 0): -1.0, (4, 4, 0): -1.0}\n"
     ]
    }
   ],
   "source": [
    "# State from l = 4\n",
    "staa = []\n",
    "x=0\n",
    "y=0\n",
    "z=0\n",
    "\n",
    "for x in range(5):\n",
    "    for y in range(5):\n",
    "        for z in range(5):\n",
    "            staa.append((x, y, z))\n",
    "\n",
    "result = list(\n",
    "    filter(\n",
    "        lambda tup: tup[2] == 0,\n",
    "        staa\n",
    "    )\n",
    ")\n",
    "\n",
    "# rewards and punishment for terminal state spaces \n",
    "my_dict = {}\n",
    "for i in result:\n",
    "    if(i == (2,2,0)):\n",
    "        my_dict[i] = 1.0\n",
    "    else:\n",
    "        my_dict[i] = -1.0\n",
    "        \n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = -0.01\n",
    "mpd = MDPLanding(staa, my_dict, r)\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    prob = 1\n",
    "    next_move = mdp.result(s, a)\n",
    "    utility = prob * U[next_move]\n",
    "    return utility\n",
    "\n",
    "def policy_evaluation(pi, U, mdp):\n",
    "    gama = mdp.default_reward\n",
    "    for s in mdp.states:\n",
    "        R = mdp.reward(s)\n",
    "        U[s] = R + gama + (expected_utility(pi[s], s, U, mdp))\n",
    "    return U\n",
    "\n",
    "def max_unility_helper(a, s, u, mpd):\n",
    "    all_utility_from_action = {}\n",
    "    for i in range(len(a)):\n",
    "        curr_u = expected_utility(a[i], s, u, mpd)\n",
    "        all_utility_from_action[a[i]]=(curr_u)\n",
    "        \n",
    "    max_key = max(all_utility_from_action, key=all_utility_from_action.get)\n",
    "    return max_key, all_utility_from_action[max_key]\n",
    "    \n",
    "def policy_iteration(mdp):\n",
    "    U = {s: mdp.default_reward for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            t = mdp.actions(s)\n",
    "            a, tt = max_unility_helper(t, s, U, mdp)\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return pi, U\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For move (0,2,1) the best move was:  (1, 0, 0) should be East\n",
      "\n",
      "For move (2,2,1) the best move was:  (0, 0, -1) should be Down\n",
      "\n",
      "For move (2,0,1) the best move was:  (0, 1, 0) should be North\n"
     ]
    }
   ],
   "source": [
    "best_policy, best_utility = policy_iteration(mpd)\n",
    "\n",
    "print(\"For move (0,2,1) the best move was: \",best_policy[(0,2,1)], \"should be East\")\n",
    "print()\n",
    "print(\"For move (2,2,1) the best move was: \",best_policy[(2,2,1)], \"should be Down\")\n",
    "print()\n",
    "print(\"For move (2,0,1) the best move was: \",best_policy[(2,0,1)], \"should be North\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0, 0): -1.1400000000000001, (0, 0, 1): 0.8399999999999999, (0, 0, 2): 0.8199999999999998, (0, 0, 3): 0.7999999999999998, (0, 0, 4): 0.7799999999999998, (0, 1, 0): -0.15000000000000013, (0, 1, 1): 1.8299999999999998, (0, 1, 2): 1.8099999999999998, (0, 1, 3): 1.7899999999999998, (0, 1, 4): 1.7699999999999998, (0, 2, 0): 0.8599999999999999, (0, 2, 1): 2.82, (0, 2, 2): 2.8, (0, 2, 3): 2.78, (0, 2, 4): 2.76, (0, 3, 0): 0.8199999999999998, (0, 3, 1): 2.8, (0, 3, 2): 2.78, (0, 3, 3): 2.76, (0, 3, 4): 2.7399999999999998, (0, 4, 0): 0.7999999999999998, (0, 4, 1): 2.78, (0, 4, 2): 2.76, (0, 4, 3): 2.7399999999999998, (0, 4, 4): 2.7199999999999998, (1, 0, 0): -0.15000000000000013, (1, 0, 1): 1.8299999999999998, (1, 0, 2): 1.8099999999999998, (1, 0, 3): 1.7899999999999998, (1, 0, 4): 1.7699999999999998, (1, 1, 0): 0.8599999999999999, (1, 1, 1): 2.82, (1, 1, 2): 2.8, (1, 1, 3): 2.78, (1, 1, 4): 2.76, (1, 2, 0): 2.84, (1, 2, 1): 3.8099999999999996, (1, 2, 2): 3.7899999999999996, (1, 2, 3): 3.7699999999999996, (1, 2, 4): 3.7499999999999996, (1, 3, 0): 1.8299999999999998, (1, 3, 1): 3.7899999999999996, (1, 3, 2): 3.7699999999999996, (1, 3, 3): 3.7499999999999996, (1, 3, 4): 3.7299999999999995, (1, 4, 0): 1.7899999999999998, (1, 4, 1): 3.7699999999999996, (1, 4, 2): 3.7499999999999996, (1, 4, 3): 3.7299999999999995, (1, 4, 4): 3.7099999999999995, (2, 0, 0): 0.8599999999999999, (2, 0, 1): 2.82, (2, 0, 2): 2.8, (2, 0, 3): 2.78, (2, 0, 4): 2.76, (2, 1, 0): 2.84, (2, 1, 1): 3.8099999999999996, (2, 1, 2): 3.7899999999999996, (2, 1, 3): 3.7699999999999996, (2, 1, 4): 3.7499999999999996, (2, 2, 0): 4.819999999999999, (2, 2, 1): 4.8, (2, 2, 2): 4.78, (2, 2, 3): 4.760000000000001, (2, 2, 4): 4.740000000000001, (2, 3, 0): 3.8099999999999996, (2, 3, 1): 4.78, (2, 3, 2): 4.760000000000001, (2, 3, 3): 4.740000000000001, (2, 3, 4): 4.7200000000000015, (2, 4, 0): 2.7799999999999994, (2, 4, 1): 4.760000000000001, (2, 4, 2): 4.740000000000001, (2, 4, 3): 4.7200000000000015, (2, 4, 4): 4.700000000000002, (3, 0, 0): 0.8199999999999998, (3, 0, 1): 2.8, (3, 0, 2): 2.78, (3, 0, 3): 2.76, (3, 0, 4): 2.7399999999999998, (3, 1, 0): 1.8299999999999998, (3, 1, 1): 3.7899999999999996, (3, 1, 2): 3.7699999999999996, (3, 1, 3): 3.7499999999999996, (3, 1, 4): 3.7299999999999995, (3, 2, 0): 3.8099999999999996, (3, 2, 1): 4.78, (3, 2, 2): 4.760000000000001, (3, 2, 3): 4.740000000000001, (3, 2, 4): 4.7200000000000015, (3, 3, 0): 2.7799999999999994, (3, 3, 1): 4.760000000000001, (3, 3, 2): 4.740000000000001, (3, 3, 3): 4.7200000000000015, (3, 3, 4): 4.700000000000002, (3, 4, 0): 2.76, (3, 4, 1): 4.740000000000001, (3, 4, 2): 4.7200000000000015, (3, 4, 3): 4.700000000000002, (3, 4, 4): 4.680000000000002, (4, 0, 0): -1.1400000000000001, (4, 0, 1): 2.78, (4, 0, 2): 2.76, (4, 0, 3): 2.7399999999999998, (4, 0, 4): 2.7199999999999998, (4, 1, 0): 1.7899999999999998, (4, 1, 1): 3.7699999999999996, (4, 1, 2): 3.7499999999999996, (4, 1, 3): 3.7299999999999995, (4, 1, 4): 3.7099999999999995, (4, 2, 0): 2.7799999999999994, (4, 2, 1): 4.760000000000001, (4, 2, 2): 4.740000000000001, (4, 2, 3): 4.7200000000000015, (4, 2, 4): 4.700000000000002, (4, 3, 0): 2.76, (4, 3, 1): 4.740000000000001, (4, 3, 2): 4.7200000000000015, (4, 3, 3): 4.700000000000002, (4, 3, 4): 4.680000000000002, (4, 4, 0): 2.7399999999999993, (4, 4, 1): 4.7200000000000015, (4, 4, 2): 4.700000000000002, (4, 4, 3): 4.680000000000002, (4, 4, 4): 4.660000000000003}\n"
     ]
    }
   ],
   "source": [
    "print(best_utility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "Provide an example of a non-deterministic transition that could be included in your code in Part C. Describe the function. How would you modify your code to handle a non-deterministic transition function?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n",
    "`Our drone flys in a non-deterministic motion, so there is not probability involed with its movements. The way our code ensure this is with the calculation of Expected utility. Normally we would sum the probability of choose our next action but because there is already a chossen next action, we dont need to include the probabily asspect. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "Describe the main differences between **policy iteration** and **value iteration**? How would your code change in Part C to convert it to **value iteration**?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "`Value iteration keeps track of utilities and will terminate when updates are at some thresh hold. Policy iteration also stores policy and will terminate when the policy terminates. In our code we would have to convert our calculation to store the next move. We would insted of chooseing the next utility based on random action through out the board, we will run through every possible action avaliable to us in that given state and apply a greedy algorithm to store the best possible action to choose. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part F\n",
    "\n",
    "Code up a **Q-learning** agent/algorithm to learn how to land the drone. You can do this however you like, as long as you use the MDP class structure defined above.  \n",
    "\n",
    "Your code should include some kind of a wrapper to run many trials to train the agent and learn the Q values.  You also do not need to have a separate function for the actual \"agent\"; your code can just be a \"for\" loop within which you are refining your estimate of the Q values.\n",
    "\n",
    "From each training trial, save the cumulative discounted reward (utility) over the course of that episode. That is, add up all of $\\gamma^t R(s_t)$ where the drone is in state $s_t$ during time step $t$, for the entire sequence. We refer to this as \"cumulative reward\" because we usually refer to \"utility\" as the utility *under an optimal policy*.\n",
    "\n",
    "Some guidelines:\n",
    "* The drone should initialize in a random non-terminal state for each new training episode.\n",
    "* The training episodes should be limited to 50 time steps, even if the drone has not yet landed. If the drone lands (in a terminal state), the training episode is over.\n",
    "* You may use whatever learning rate $\\alpha$ you decide is appropriate, and gives good results.\n",
    "* There are many forms of Q-learning. You can use whatever you would like, subject to the reliability targets below.\n",
    "* Your code should return:\n",
    "  * The learned Q values associated with each state-action pair.\n",
    "  * The cumulative reward for each training trial. \n",
    "  * Anything else that might be useful in the ensuing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 1), (0, 0, 2), (0, 0, 3), (0, 0, 4), (0, 1, 1), (0, 1, 2), (0, 1, 3), (0, 1, 4), (0, 2, 1), (0, 2, 2), (0, 2, 3), (0, 2, 4), (0, 3, 1), (0, 3, 2), (0, 3, 3), (0, 3, 4), (0, 4, 1), (0, 4, 2), (0, 4, 3), (0, 4, 4), (1, 0, 1), (1, 0, 2), (1, 0, 3), (1, 0, 4), (1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4), (1, 2, 1), (1, 2, 2), (1, 2, 3), (1, 2, 4), (1, 3, 1), (1, 3, 2), (1, 3, 3), (1, 3, 4), (1, 4, 1), (1, 4, 2), (1, 4, 3), (1, 4, 4), (2, 0, 1), (2, 0, 2), (2, 0, 3), (2, 0, 4), (2, 1, 1), (2, 1, 2), (2, 1, 3), (2, 1, 4), (2, 2, 1), (2, 2, 2), (2, 2, 3), (2, 2, 4), (2, 3, 1), (2, 3, 2), (2, 3, 3), (2, 3, 4), (2, 4, 1), (2, 4, 2), (2, 4, 3), (2, 4, 4), (3, 0, 1), (3, 0, 2), (3, 0, 3), (3, 0, 4), (3, 1, 1), (3, 1, 2), (3, 1, 3), (3, 1, 4), (3, 2, 1), (3, 2, 2), (3, 2, 3), (3, 2, 4), (3, 3, 1), (3, 3, 2), (3, 3, 3), (3, 3, 4), (3, 4, 1), (3, 4, 2), (3, 4, 3), (3, 4, 4), (4, 0, 1), (4, 0, 2), (4, 0, 3), (4, 0, 4), (4, 1, 1), (4, 1, 2), (4, 1, 3), (4, 1, 4), (4, 2, 1), (4, 2, 2), (4, 2, 3), (4, 2, 4), (4, 3, 1), (4, 3, 2), (4, 3, 3), (4, 3, 4), (4, 4, 1), (4, 4, 2), (4, 4, 3), (4, 4, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Solution:\n",
    "results = [t for t in mpd.states if t[2] > 0]\n",
    "print(results)\n",
    "\n",
    "def temporal_difference(s,a, mdp, u, poli, iteration):\n",
    "    t = mdp.result(s,a)\n",
    "    list_ac = mdp.actions(s)\n",
    "    max_t, tt_u = max_unility_helper(list_ac, s, u, mpd)\n",
    "    at = 60/59+iteration\n",
    "    r = mdp.reward(t)\n",
    "    adjust = tt_u - u[t]\n",
    "    temporal = at * (r + mdp.discount*(adjust))\n",
    "    return max_t, temporal\n",
    "    \n",
    "def Q_learning(mdp, inital_policy):\n",
    "    Q = {}\n",
    "    ittter = 50\n",
    "    for i in range(200):\n",
    "        new_start = random.choice(results)\n",
    "        print(new_start)\n",
    "        for x in range(50):\n",
    "            if (new_start in mpd.terminal_states):\n",
    "                return Q\n",
    "            else:\n",
    "                # pick an action using greedy \n",
    "                next_act = inital_policy[new_start]\n",
    "                # count stat-action pair \n",
    "                \n",
    "                # calculate Q[s,a] for action selected\n",
    "                tempo_p, tempo_u = temporal_difference(new_start, next_act, mdp, best_utility, inital_policy,ittter)\n",
    "                \n",
    "                # update policy for action at Q[s,a]\n",
    "                inital_policy[new_start] = tempo_p\n",
    "                \n",
    "                #calculat commulative rewards using discound, step, rcs(reward matrix)\n",
    "                tempo_u\n",
    "        Q[new_start] = tempo_u\n",
    "        ittter = ittter+1\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 2)\n",
      "(2, 2, 1)\n",
      "(4, 3, 1)\n",
      "(0, 2, 1)\n",
      "(3, 3, 1)\n",
      "(1, 2, 3)\n",
      "(2, 4, 3)\n",
      "(3, 4, 1)\n",
      "(1, 4, 4)\n",
      "(3, 4, 4)\n",
      "(1, 1, 4)\n",
      "(4, 0, 1)\n",
      "(2, 1, 3)\n",
      "(0, 1, 4)\n",
      "(4, 1, 4)\n",
      "(0, 2, 4)\n",
      "(4, 0, 4)\n",
      "(1, 3, 2)\n",
      "(4, 1, 1)\n",
      "(0, 1, 2)\n",
      "(4, 2, 1)\n",
      "(0, 3, 4)\n",
      "(4, 4, 4)\n",
      "(2, 0, 4)\n",
      "(4, 1, 4)\n",
      "(2, 3, 4)\n",
      "(2, 4, 4)\n",
      "(2, 2, 2)\n",
      "(1, 1, 3)\n",
      "(1, 0, 3)\n",
      "(0, 4, 1)\n",
      "(0, 3, 1)\n",
      "(4, 0, 1)\n",
      "(0, 3, 1)\n",
      "(0, 1, 3)\n",
      "(3, 1, 2)\n",
      "(0, 3, 2)\n",
      "(2, 3, 3)\n",
      "(1, 1, 3)\n",
      "(2, 3, 2)\n",
      "(0, 1, 4)\n",
      "(2, 2, 2)\n",
      "(4, 0, 4)\n",
      "(0, 4, 4)\n",
      "(3, 4, 1)\n",
      "(1, 1, 4)\n",
      "(0, 0, 4)\n",
      "(3, 4, 2)\n",
      "(1, 2, 4)\n",
      "(2, 4, 2)\n",
      "(1, 0, 3)\n",
      "(1, 0, 1)\n",
      "(4, 4, 1)\n",
      "(0, 1, 3)\n",
      "(0, 1, 3)\n",
      "(2, 4, 4)\n",
      "(0, 1, 4)\n",
      "(2, 0, 4)\n",
      "(2, 3, 2)\n",
      "(2, 4, 2)\n",
      "(4, 4, 4)\n",
      "(0, 1, 2)\n",
      "(2, 3, 2)\n",
      "(0, 0, 3)\n",
      "(0, 1, 2)\n",
      "(4, 1, 1)\n",
      "(0, 0, 1)\n",
      "(4, 2, 1)\n",
      "(4, 4, 4)\n",
      "(0, 3, 1)\n",
      "(1, 0, 1)\n",
      "(3, 0, 4)\n",
      "(4, 1, 4)\n",
      "(3, 2, 2)\n",
      "(2, 0, 2)\n",
      "(3, 2, 3)\n",
      "(3, 0, 4)\n",
      "(1, 0, 1)\n",
      "(2, 0, 2)\n",
      "(3, 3, 4)\n",
      "(3, 1, 1)\n",
      "(3, 1, 3)\n",
      "(3, 0, 2)\n",
      "(0, 2, 1)\n",
      "(4, 1, 1)\n",
      "(4, 4, 1)\n",
      "(1, 1, 4)\n",
      "(2, 1, 3)\n",
      "(1, 3, 1)\n",
      "(3, 4, 2)\n",
      "(3, 0, 2)\n",
      "(2, 3, 4)\n",
      "(0, 4, 4)\n",
      "(4, 1, 4)\n",
      "(0, 2, 2)\n",
      "(3, 0, 3)\n",
      "(4, 2, 4)\n",
      "(2, 1, 4)\n",
      "(0, 1, 2)\n",
      "(0, 3, 4)\n",
      "(0, 0, 3)\n",
      "(2, 3, 1)\n",
      "(3, 4, 3)\n",
      "(0, 3, 1)\n",
      "(2, 0, 2)\n",
      "(1, 2, 2)\n",
      "(0, 0, 2)\n",
      "(0, 3, 2)\n",
      "(0, 2, 3)\n",
      "(1, 4, 1)\n",
      "(3, 1, 2)\n",
      "(2, 2, 1)\n",
      "(4, 4, 1)\n",
      "(0, 3, 4)\n",
      "(1, 0, 2)\n",
      "(2, 4, 1)\n",
      "(1, 3, 4)\n",
      "(1, 4, 4)\n",
      "(3, 3, 3)\n",
      "(2, 2, 3)\n",
      "(4, 2, 3)\n",
      "(2, 1, 2)\n",
      "(1, 1, 4)\n",
      "(1, 1, 4)\n",
      "(2, 0, 2)\n",
      "(3, 1, 4)\n",
      "(3, 1, 2)\n",
      "(2, 1, 3)\n",
      "(3, 2, 4)\n",
      "(1, 2, 2)\n",
      "(4, 4, 2)\n",
      "(0, 3, 4)\n",
      "(1, 3, 4)\n",
      "(3, 4, 4)\n",
      "(2, 0, 3)\n",
      "(4, 3, 1)\n",
      "(4, 0, 1)\n",
      "(1, 3, 3)\n",
      "(3, 0, 4)\n",
      "(2, 3, 2)\n",
      "(2, 0, 1)\n",
      "(2, 2, 2)\n",
      "(2, 3, 4)\n",
      "(1, 4, 1)\n",
      "(4, 0, 2)\n",
      "(4, 4, 1)\n",
      "(3, 4, 4)\n",
      "(4, 1, 2)\n",
      "(3, 3, 1)\n",
      "(4, 4, 2)\n",
      "(1, 0, 2)\n",
      "(1, 0, 3)\n",
      "(4, 2, 3)\n",
      "(3, 3, 2)\n",
      "(2, 3, 4)\n",
      "(3, 1, 4)\n",
      "(0, 4, 1)\n",
      "(3, 2, 3)\n",
      "(2, 4, 1)\n",
      "(4, 0, 4)\n",
      "(3, 1, 4)\n",
      "(1, 2, 1)\n",
      "(3, 1, 1)\n",
      "(4, 1, 1)\n",
      "(1, 2, 4)\n",
      "(0, 4, 4)\n",
      "(3, 3, 4)\n",
      "(3, 4, 1)\n",
      "(0, 2, 4)\n",
      "(4, 1, 2)\n",
      "(4, 4, 4)\n",
      "(4, 2, 3)\n",
      "(3, 1, 4)\n",
      "(4, 2, 3)\n",
      "(4, 0, 2)\n",
      "(1, 0, 1)\n",
      "(0, 2, 3)\n",
      "(4, 4, 4)\n",
      "(3, 0, 2)\n",
      "(3, 4, 3)\n",
      "(2, 1, 1)\n",
      "(3, 0, 3)\n",
      "(0, 4, 4)\n",
      "(4, 0, 2)\n",
      "(2, 1, 3)\n",
      "(3, 1, 2)\n",
      "(4, 1, 3)\n",
      "(4, 1, 1)\n",
      "(0, 1, 1)\n",
      "(3, 0, 4)\n",
      "(4, 4, 3)\n",
      "(1, 1, 4)\n",
      "(0, 0, 3)\n",
      "(1, 0, 3)\n",
      "(1, 3, 2)\n",
      "(4, 3, 1)\n",
      "(0, 1, 2)\n",
      "(3, 3, 2)\n",
      "(0, 2, 4)\n",
      "(1, 4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(4, 1, 2): -2.2001694915254237,\n",
       " (2, 2, 1): 162.01694915254237,\n",
       " (4, 3, 1): -2.4601694915254235,\n",
       " (0, 2, 1): -1.3401694915254236,\n",
       " (3, 3, 1): -1.9901694915254238,\n",
       " (1, 2, 3): -0.5601694915254237,\n",
       " (2, 4, 3): -0.5701694915254237,\n",
       " (3, 4, 1): -2.1801694915254237,\n",
       " (1, 4, 4): -1.6801694915254237,\n",
       " (3, 4, 4): -1.9701694915254238,\n",
       " (1, 1, 4): -2.420169491525424,\n",
       " (4, 0, 1): -1.8701694915254237,\n",
       " (2, 1, 3): -2.3501694915254236,\n",
       " (0, 1, 4): -1.0701694915254236,\n",
       " (4, 1, 4): -1.4401694915254237,\n",
       " (0, 2, 4): -2.4901694915254238,\n",
       " (4, 0, 4): -2.1001694915254236,\n",
       " (1, 3, 2): -2.4501694915254237,\n",
       " (4, 1, 1): -2.380169491525424,\n",
       " (0, 1, 2): -2.4701694915254238,\n",
       " (4, 2, 1): -1.1801694915254237,\n",
       " (0, 3, 4): -1.8201694915254236,\n",
       " (4, 4, 4): -2.280169491525424,\n",
       " (2, 0, 4): -1.0801694915254236,\n",
       " (2, 3, 4): -2.050169491525424,\n",
       " (2, 4, 4): -1.0601694915254236,\n",
       " (2, 2, 2): -1.9201694915254237,\n",
       " (1, 1, 3): -0.8901694915254237,\n",
       " (1, 0, 3): -2.4401694915254235,\n",
       " (0, 4, 1): -2.070169491525424,\n",
       " (0, 3, 1): -1.5401694915254236,\n",
       " (0, 1, 3): -1.0501694915254236,\n",
       " (3, 1, 2): -2.360169491525424,\n",
       " (0, 3, 2): -1.5801694915254236,\n",
       " (2, 3, 3): -0.8801694915254237,\n",
       " (2, 3, 2): -1.9001694915254237,\n",
       " (0, 4, 4): -2.3301694915254236,\n",
       " (0, 0, 4): -0.9701694915254236,\n",
       " (3, 4, 2): -1.4001694915254237,\n",
       " (1, 2, 4): -2.150169491525424,\n",
       " (2, 4, 2): -1.1001694915254236,\n",
       " (1, 0, 1): -2.260169491525424,\n",
       " (4, 4, 1): -1.9601694915254237,\n",
       " (0, 0, 3): -2.4301694915254237,\n",
       " (0, 0, 1): -1.1701694915254237,\n",
       " (3, 0, 4): -2.400169491525424,\n",
       " (3, 2, 2): -1.2401694915254238,\n",
       " (2, 0, 2): -1.7501694915254238,\n",
       " (3, 2, 3): -2.0801694915254236,\n",
       " (3, 3, 4): -2.170169491525424,\n",
       " (3, 1, 1): -2.130169491525424,\n",
       " (3, 1, 3): -1.3201694915254236,\n",
       " (3, 0, 2): -2.2901694915254236,\n",
       " (1, 3, 1): -1.3901694915254237,\n",
       " (0, 2, 2): -1.4501694915254237,\n",
       " (3, 0, 3): -2.320169491525424,\n",
       " (4, 2, 4): -1.4701694915254238,\n",
       " (2, 1, 4): -1.4801694915254238,\n",
       " (2, 3, 1): -1.5201694915254238,\n",
       " (3, 4, 3): -2.300169491525424,\n",
       " (1, 2, 2): -1.8001694915254236,\n",
       " (0, 0, 2): -1.5701694915254236,\n",
       " (0, 2, 3): -2.2701694915254236,\n",
       " (1, 4, 1): -1.9401694915254237,\n",
       " (1, 0, 2): -2.010169491525424,\n",
       " (2, 4, 1): -2.090169491525424,\n",
       " (1, 3, 4): -1.8301694915254236,\n",
       " (3, 3, 3): -1.6901694915254237,\n",
       " (2, 2, 3): -1.7001694915254237,\n",
       " (4, 2, 3): -2.2401694915254238,\n",
       " (2, 1, 2): -1.7201694915254238,\n",
       " (3, 1, 4): -2.2301694915254235,\n",
       " (3, 2, 4): -1.7901694915254236,\n",
       " (4, 4, 2): -2.0001694915254236,\n",
       " (2, 0, 3): -1.8501694915254236,\n",
       " (1, 3, 3): -1.8801694915254237,\n",
       " (2, 0, 1): -1.9101694915254237,\n",
       " (4, 0, 2): -2.340169491525424,\n",
       " (3, 3, 2): -2.4801694915254235,\n",
       " (1, 2, 1): -2.1201694915254237,\n",
       " (2, 1, 1): -2.3101694915254236,\n",
       " (4, 1, 3): -2.3701694915254237,\n",
       " (0, 1, 1): -2.3901694915254237,\n",
       " (4, 4, 3): -2.4101694915254237,\n",
       " (1, 4, 3): -2.5001694915254236}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_learning(mpd, best_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part G\n",
    "\n",
    "Initialize the $L=10$ environment (so that the landing pad is at $(5,5,0)$). Run some number of training trials to train the drone.\n",
    "\n",
    "**How do I know if my drone is learned enough?**  If you take the mean cumulative reward across the last 5000 training trials, it should be around 0.80. This means at least about 10,000 (but probably more) training episodes will be necessary. It will take a few seconds on your computer, so start small to test your code.\n",
    "\n",
    "**Then:** Compute block means of cumulative reward from all of your training trials. Use blocks of 500 training trials. This means you need to create some kind of array-like structure such that its first element is the mean of the first 500 trials' cumulative rewards; its second element is the mean of the 501-1000th trials' cumulative rewards; and so on. Make a plot of the block mean rewards as the training progresses. It should increase from about -0.5 initially to somewhere around +0.8.\n",
    "\n",
    "**And:** Print to the screen the mean of the last 5000 trials' cumulative rewards, to verify that it is indeed about 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=10 [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 3), (0, 0, 4), (0, 0, 5), (0, 0, 6), (0, 0, 7), (0, 0, 8), (0, 0, 9), (0, 0, 10), (0, 1, 0), (0, 1, 1), (0, 1, 2), (0, 1, 3), (0, 1, 4), (0, 1, 5), (0, 1, 6), (0, 1, 7), (0, 1, 8), (0, 1, 9), (0, 1, 10), (0, 2, 0), (0, 2, 1), (0, 2, 2), (0, 2, 3), (0, 2, 4), (0, 2, 5), (0, 2, 6), (0, 2, 7), (0, 2, 8), (0, 2, 9), (0, 2, 10), (0, 3, 0), (0, 3, 1), (0, 3, 2), (0, 3, 3), (0, 3, 4), (0, 3, 5), (0, 3, 6), (0, 3, 7), (0, 3, 8), (0, 3, 9), (0, 3, 10), (0, 4, 0), (0, 4, 1), (0, 4, 2), (0, 4, 3), (0, 4, 4), (0, 4, 5), (0, 4, 6), (0, 4, 7), (0, 4, 8), (0, 4, 9), (0, 4, 10), (0, 5, 0), (0, 5, 1), (0, 5, 2), (0, 5, 3), (0, 5, 4), (0, 5, 5), (0, 5, 6), (0, 5, 7), (0, 5, 8), (0, 5, 9), (0, 5, 10), (0, 6, 0), (0, 6, 1), (0, 6, 2), (0, 6, 3), (0, 6, 4), (0, 6, 5), (0, 6, 6), (0, 6, 7), (0, 6, 8), (0, 6, 9), (0, 6, 10), (0, 7, 0), (0, 7, 1), (0, 7, 2), (0, 7, 3), (0, 7, 4), (0, 7, 5), (0, 7, 6), (0, 7, 7), (0, 7, 8), (0, 7, 9), (0, 7, 10), (0, 8, 0), (0, 8, 1), (0, 8, 2), (0, 8, 3), (0, 8, 4), (0, 8, 5), (0, 8, 6), (0, 8, 7), (0, 8, 8), (0, 8, 9), (0, 8, 10), (0, 9, 0), (0, 9, 1), (0, 9, 2), (0, 9, 3), (0, 9, 4), (0, 9, 5), (0, 9, 6), (0, 9, 7), (0, 9, 8), (0, 9, 9), (0, 9, 10), (0, 10, 0), (0, 10, 1), (0, 10, 2), (0, 10, 3), (0, 10, 4), (0, 10, 5), (0, 10, 6), (0, 10, 7), (0, 10, 8), (0, 10, 9), (0, 10, 10), (1, 0, 0), (1, 0, 1), (1, 0, 2), (1, 0, 3), (1, 0, 4), (1, 0, 5), (1, 0, 6), (1, 0, 7), (1, 0, 8), (1, 0, 9), (1, 0, 10), (1, 1, 0), (1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4), (1, 1, 5), (1, 1, 6), (1, 1, 7), (1, 1, 8), (1, 1, 9), (1, 1, 10), (1, 2, 0), (1, 2, 1), (1, 2, 2), (1, 2, 3), (1, 2, 4), (1, 2, 5), (1, 2, 6), (1, 2, 7), (1, 2, 8), (1, 2, 9), (1, 2, 10), (1, 3, 0), (1, 3, 1), (1, 3, 2), (1, 3, 3), (1, 3, 4), (1, 3, 5), (1, 3, 6), (1, 3, 7), (1, 3, 8), (1, 3, 9), (1, 3, 10), (1, 4, 0), (1, 4, 1), (1, 4, 2), (1, 4, 3), (1, 4, 4), (1, 4, 5), (1, 4, 6), (1, 4, 7), (1, 4, 8), (1, 4, 9), (1, 4, 10), (1, 5, 0), (1, 5, 1), (1, 5, 2), (1, 5, 3), (1, 5, 4), (1, 5, 5), (1, 5, 6), (1, 5, 7), (1, 5, 8), (1, 5, 9), (1, 5, 10), (1, 6, 0), (1, 6, 1), (1, 6, 2), (1, 6, 3), (1, 6, 4), (1, 6, 5), (1, 6, 6), (1, 6, 7), (1, 6, 8), (1, 6, 9), (1, 6, 10), (1, 7, 0), (1, 7, 1), (1, 7, 2), (1, 7, 3), (1, 7, 4), (1, 7, 5), (1, 7, 6), (1, 7, 7), (1, 7, 8), (1, 7, 9), (1, 7, 10), (1, 8, 0), (1, 8, 1), (1, 8, 2), (1, 8, 3), (1, 8, 4), (1, 8, 5), (1, 8, 6), (1, 8, 7), (1, 8, 8), (1, 8, 9), (1, 8, 10), (1, 9, 0), (1, 9, 1), (1, 9, 2), (1, 9, 3), (1, 9, 4), (1, 9, 5), (1, 9, 6), (1, 9, 7), (1, 9, 8), (1, 9, 9), (1, 9, 10), (1, 10, 0), (1, 10, 1), (1, 10, 2), (1, 10, 3), (1, 10, 4), (1, 10, 5), (1, 10, 6), (1, 10, 7), (1, 10, 8), (1, 10, 9), (1, 10, 10), (2, 0, 0), (2, 0, 1), (2, 0, 2), (2, 0, 3), (2, 0, 4), (2, 0, 5), (2, 0, 6), (2, 0, 7), (2, 0, 8), (2, 0, 9), (2, 0, 10), (2, 1, 0), (2, 1, 1), (2, 1, 2), (2, 1, 3), (2, 1, 4), (2, 1, 5), (2, 1, 6), (2, 1, 7), (2, 1, 8), (2, 1, 9), (2, 1, 10), (2, 2, 0), (2, 2, 1), (2, 2, 2), (2, 2, 3), (2, 2, 4), (2, 2, 5), (2, 2, 6), (2, 2, 7), (2, 2, 8), (2, 2, 9), (2, 2, 10), (2, 3, 0), (2, 3, 1), (2, 3, 2), (2, 3, 3), (2, 3, 4), (2, 3, 5), (2, 3, 6), (2, 3, 7), (2, 3, 8), (2, 3, 9), (2, 3, 10), (2, 4, 0), (2, 4, 1), (2, 4, 2), (2, 4, 3), (2, 4, 4), (2, 4, 5), (2, 4, 6), (2, 4, 7), (2, 4, 8), (2, 4, 9), (2, 4, 10), (2, 5, 0), (2, 5, 1), (2, 5, 2), (2, 5, 3), (2, 5, 4), (2, 5, 5), (2, 5, 6), (2, 5, 7), (2, 5, 8), (2, 5, 9), (2, 5, 10), (2, 6, 0), (2, 6, 1), (2, 6, 2), (2, 6, 3), (2, 6, 4), (2, 6, 5), (2, 6, 6), (2, 6, 7), (2, 6, 8), (2, 6, 9), (2, 6, 10), (2, 7, 0), (2, 7, 1), (2, 7, 2), (2, 7, 3), (2, 7, 4), (2, 7, 5), (2, 7, 6), (2, 7, 7), (2, 7, 8), (2, 7, 9), (2, 7, 10), (2, 8, 0), (2, 8, 1), (2, 8, 2), (2, 8, 3), (2, 8, 4), (2, 8, 5), (2, 8, 6), (2, 8, 7), (2, 8, 8), (2, 8, 9), (2, 8, 10), (2, 9, 0), (2, 9, 1), (2, 9, 2), (2, 9, 3), (2, 9, 4), (2, 9, 5), (2, 9, 6), (2, 9, 7), (2, 9, 8), (2, 9, 9), (2, 9, 10), (2, 10, 0), (2, 10, 1), (2, 10, 2), (2, 10, 3), (2, 10, 4), (2, 10, 5), (2, 10, 6), (2, 10, 7), (2, 10, 8), (2, 10, 9), (2, 10, 10), (3, 0, 0), (3, 0, 1), (3, 0, 2), (3, 0, 3), (3, 0, 4), (3, 0, 5), (3, 0, 6), (3, 0, 7), (3, 0, 8), (3, 0, 9), (3, 0, 10), (3, 1, 0), (3, 1, 1), (3, 1, 2), (3, 1, 3), (3, 1, 4), (3, 1, 5), (3, 1, 6), (3, 1, 7), (3, 1, 8), (3, 1, 9), (3, 1, 10), (3, 2, 0), (3, 2, 1), (3, 2, 2), (3, 2, 3), (3, 2, 4), (3, 2, 5), (3, 2, 6), (3, 2, 7), (3, 2, 8), (3, 2, 9), (3, 2, 10), (3, 3, 0), (3, 3, 1), (3, 3, 2), (3, 3, 3), (3, 3, 4), (3, 3, 5), (3, 3, 6), (3, 3, 7), (3, 3, 8), (3, 3, 9), (3, 3, 10), (3, 4, 0), (3, 4, 1), (3, 4, 2), (3, 4, 3), (3, 4, 4), (3, 4, 5), (3, 4, 6), (3, 4, 7), (3, 4, 8), (3, 4, 9), (3, 4, 10), (3, 5, 0), (3, 5, 1), (3, 5, 2), (3, 5, 3), (3, 5, 4), (3, 5, 5), (3, 5, 6), (3, 5, 7), (3, 5, 8), (3, 5, 9), (3, 5, 10), (3, 6, 0), (3, 6, 1), (3, 6, 2), (3, 6, 3), (3, 6, 4), (3, 6, 5), (3, 6, 6), (3, 6, 7), (3, 6, 8), (3, 6, 9), (3, 6, 10), (3, 7, 0), (3, 7, 1), (3, 7, 2), (3, 7, 3), (3, 7, 4), (3, 7, 5), (3, 7, 6), (3, 7, 7), (3, 7, 8), (3, 7, 9), (3, 7, 10), (3, 8, 0), (3, 8, 1), (3, 8, 2), (3, 8, 3), (3, 8, 4), (3, 8, 5), (3, 8, 6), (3, 8, 7), (3, 8, 8), (3, 8, 9), (3, 8, 10), (3, 9, 0), (3, 9, 1), (3, 9, 2), (3, 9, 3), (3, 9, 4), (3, 9, 5), (3, 9, 6), (3, 9, 7), (3, 9, 8), (3, 9, 9), (3, 9, 10), (3, 10, 0), (3, 10, 1), (3, 10, 2), (3, 10, 3), (3, 10, 4), (3, 10, 5), (3, 10, 6), (3, 10, 7), (3, 10, 8), (3, 10, 9), (3, 10, 10), (4, 0, 0), (4, 0, 1), (4, 0, 2), (4, 0, 3), (4, 0, 4), (4, 0, 5), (4, 0, 6), (4, 0, 7), (4, 0, 8), (4, 0, 9), (4, 0, 10), (4, 1, 0), (4, 1, 1), (4, 1, 2), (4, 1, 3), (4, 1, 4), (4, 1, 5), (4, 1, 6), (4, 1, 7), (4, 1, 8), (4, 1, 9), (4, 1, 10), (4, 2, 0), (4, 2, 1), (4, 2, 2), (4, 2, 3), (4, 2, 4), (4, 2, 5), (4, 2, 6), (4, 2, 7), (4, 2, 8), (4, 2, 9), (4, 2, 10), (4, 3, 0), (4, 3, 1), (4, 3, 2), (4, 3, 3), (4, 3, 4), (4, 3, 5), (4, 3, 6), (4, 3, 7), (4, 3, 8), (4, 3, 9), (4, 3, 10), (4, 4, 0), (4, 4, 1), (4, 4, 2), (4, 4, 3), (4, 4, 4), (4, 4, 5), (4, 4, 6), (4, 4, 7), (4, 4, 8), (4, 4, 9), (4, 4, 10), (4, 5, 0), (4, 5, 1), (4, 5, 2), (4, 5, 3), (4, 5, 4), (4, 5, 5), (4, 5, 6), (4, 5, 7), (4, 5, 8), (4, 5, 9), (4, 5, 10), (4, 6, 0), (4, 6, 1), (4, 6, 2), (4, 6, 3), (4, 6, 4), (4, 6, 5), (4, 6, 6), (4, 6, 7), (4, 6, 8), (4, 6, 9), (4, 6, 10), (4, 7, 0), (4, 7, 1), (4, 7, 2), (4, 7, 3), (4, 7, 4), (4, 7, 5), (4, 7, 6), (4, 7, 7), (4, 7, 8), (4, 7, 9), (4, 7, 10), (4, 8, 0), (4, 8, 1), (4, 8, 2), (4, 8, 3), (4, 8, 4), (4, 8, 5), (4, 8, 6), (4, 8, 7), (4, 8, 8), (4, 8, 9), (4, 8, 10), (4, 9, 0), (4, 9, 1), (4, 9, 2), (4, 9, 3), (4, 9, 4), (4, 9, 5), (4, 9, 6), (4, 9, 7), (4, 9, 8), (4, 9, 9), (4, 9, 10), (4, 10, 0), (4, 10, 1), (4, 10, 2), (4, 10, 3), (4, 10, 4), (4, 10, 5), (4, 10, 6), (4, 10, 7), (4, 10, 8), (4, 10, 9), (4, 10, 10), (5, 0, 0), (5, 0, 1), (5, 0, 2), (5, 0, 3), (5, 0, 4), (5, 0, 5), (5, 0, 6), (5, 0, 7), (5, 0, 8), (5, 0, 9), (5, 0, 10), (5, 1, 0), (5, 1, 1), (5, 1, 2), (5, 1, 3), (5, 1, 4), (5, 1, 5), (5, 1, 6), (5, 1, 7), (5, 1, 8), (5, 1, 9), (5, 1, 10), (5, 2, 0), (5, 2, 1), (5, 2, 2), (5, 2, 3), (5, 2, 4), (5, 2, 5), (5, 2, 6), (5, 2, 7), (5, 2, 8), (5, 2, 9), (5, 2, 10), (5, 3, 0), (5, 3, 1), (5, 3, 2), (5, 3, 3), (5, 3, 4), (5, 3, 5), (5, 3, 6), (5, 3, 7), (5, 3, 8), (5, 3, 9), (5, 3, 10), (5, 4, 0), (5, 4, 1), (5, 4, 2), (5, 4, 3), (5, 4, 4), (5, 4, 5), (5, 4, 6), (5, 4, 7), (5, 4, 8), (5, 4, 9), (5, 4, 10), (5, 5, 0), (5, 5, 1), (5, 5, 2), (5, 5, 3), (5, 5, 4), (5, 5, 5), (5, 5, 6), (5, 5, 7), (5, 5, 8), (5, 5, 9), (5, 5, 10), (5, 6, 0), (5, 6, 1), (5, 6, 2), (5, 6, 3), (5, 6, 4), (5, 6, 5), (5, 6, 6), (5, 6, 7), (5, 6, 8), (5, 6, 9), (5, 6, 10), (5, 7, 0), (5, 7, 1), (5, 7, 2), (5, 7, 3), (5, 7, 4), (5, 7, 5), (5, 7, 6), (5, 7, 7), (5, 7, 8), (5, 7, 9), (5, 7, 10), (5, 8, 0), (5, 8, 1), (5, 8, 2), (5, 8, 3), (5, 8, 4), (5, 8, 5), (5, 8, 6), (5, 8, 7), (5, 8, 8), (5, 8, 9), (5, 8, 10), (5, 9, 0), (5, 9, 1), (5, 9, 2), (5, 9, 3), (5, 9, 4), (5, 9, 5), (5, 9, 6), (5, 9, 7), (5, 9, 8), (5, 9, 9), (5, 9, 10), (5, 10, 0), (5, 10, 1), (5, 10, 2), (5, 10, 3), (5, 10, 4), (5, 10, 5), (5, 10, 6), (5, 10, 7), (5, 10, 8), (5, 10, 9), (5, 10, 10), (6, 0, 0), (6, 0, 1), (6, 0, 2), (6, 0, 3), (6, 0, 4), (6, 0, 5), (6, 0, 6), (6, 0, 7), (6, 0, 8), (6, 0, 9), (6, 0, 10), (6, 1, 0), (6, 1, 1), (6, 1, 2), (6, 1, 3), (6, 1, 4), (6, 1, 5), (6, 1, 6), (6, 1, 7), (6, 1, 8), (6, 1, 9), (6, 1, 10), (6, 2, 0), (6, 2, 1), (6, 2, 2), (6, 2, 3), (6, 2, 4), (6, 2, 5), (6, 2, 6), (6, 2, 7), (6, 2, 8), (6, 2, 9), (6, 2, 10), (6, 3, 0), (6, 3, 1), (6, 3, 2), (6, 3, 3), (6, 3, 4), (6, 3, 5), (6, 3, 6), (6, 3, 7), (6, 3, 8), (6, 3, 9), (6, 3, 10), (6, 4, 0), (6, 4, 1), (6, 4, 2), (6, 4, 3), (6, 4, 4), (6, 4, 5), (6, 4, 6), (6, 4, 7), (6, 4, 8), (6, 4, 9), (6, 4, 10), (6, 5, 0), (6, 5, 1), (6, 5, 2), (6, 5, 3), (6, 5, 4), (6, 5, 5), (6, 5, 6), (6, 5, 7), (6, 5, 8), (6, 5, 9), (6, 5, 10), (6, 6, 0), (6, 6, 1), (6, 6, 2), (6, 6, 3), (6, 6, 4), (6, 6, 5), (6, 6, 6), (6, 6, 7), (6, 6, 8), (6, 6, 9), (6, 6, 10), (6, 7, 0), (6, 7, 1), (6, 7, 2), (6, 7, 3), (6, 7, 4), (6, 7, 5), (6, 7, 6), (6, 7, 7), (6, 7, 8), (6, 7, 9), (6, 7, 10), (6, 8, 0), (6, 8, 1), (6, 8, 2), (6, 8, 3), (6, 8, 4), (6, 8, 5), (6, 8, 6), (6, 8, 7), (6, 8, 8), (6, 8, 9), (6, 8, 10), (6, 9, 0), (6, 9, 1), (6, 9, 2), (6, 9, 3), (6, 9, 4), (6, 9, 5), (6, 9, 6), (6, 9, 7), (6, 9, 8), (6, 9, 9), (6, 9, 10), (6, 10, 0), (6, 10, 1), (6, 10, 2), (6, 10, 3), (6, 10, 4), (6, 10, 5), (6, 10, 6), (6, 10, 7), (6, 10, 8), (6, 10, 9), (6, 10, 10), (7, 0, 0), (7, 0, 1), (7, 0, 2), (7, 0, 3), (7, 0, 4), (7, 0, 5), (7, 0, 6), (7, 0, 7), (7, 0, 8), (7, 0, 9), (7, 0, 10), (7, 1, 0), (7, 1, 1), (7, 1, 2), (7, 1, 3), (7, 1, 4), (7, 1, 5), (7, 1, 6), (7, 1, 7), (7, 1, 8), (7, 1, 9), (7, 1, 10), (7, 2, 0), (7, 2, 1), (7, 2, 2), (7, 2, 3), (7, 2, 4), (7, 2, 5), (7, 2, 6), (7, 2, 7), (7, 2, 8), (7, 2, 9), (7, 2, 10), (7, 3, 0), (7, 3, 1), (7, 3, 2), (7, 3, 3), (7, 3, 4), (7, 3, 5), (7, 3, 6), (7, 3, 7), (7, 3, 8), (7, 3, 9), (7, 3, 10), (7, 4, 0), (7, 4, 1), (7, 4, 2), (7, 4, 3), (7, 4, 4), (7, 4, 5), (7, 4, 6), (7, 4, 7), (7, 4, 8), (7, 4, 9), (7, 4, 10), (7, 5, 0), (7, 5, 1), (7, 5, 2), (7, 5, 3), (7, 5, 4), (7, 5, 5), (7, 5, 6), (7, 5, 7), (7, 5, 8), (7, 5, 9), (7, 5, 10), (7, 6, 0), (7, 6, 1), (7, 6, 2), (7, 6, 3), (7, 6, 4), (7, 6, 5), (7, 6, 6), (7, 6, 7), (7, 6, 8), (7, 6, 9), (7, 6, 10), (7, 7, 0), (7, 7, 1), (7, 7, 2), (7, 7, 3), (7, 7, 4), (7, 7, 5), (7, 7, 6), (7, 7, 7), (7, 7, 8), (7, 7, 9), (7, 7, 10), (7, 8, 0), (7, 8, 1), (7, 8, 2), (7, 8, 3), (7, 8, 4), (7, 8, 5), (7, 8, 6), (7, 8, 7), (7, 8, 8), (7, 8, 9), (7, 8, 10), (7, 9, 0), (7, 9, 1), (7, 9, 2), (7, 9, 3), (7, 9, 4), (7, 9, 5), (7, 9, 6), (7, 9, 7), (7, 9, 8), (7, 9, 9), (7, 9, 10), (7, 10, 0), (7, 10, 1), (7, 10, 2), (7, 10, 3), (7, 10, 4), (7, 10, 5), (7, 10, 6), (7, 10, 7), (7, 10, 8), (7, 10, 9), (7, 10, 10), (8, 0, 0), (8, 0, 1), (8, 0, 2), (8, 0, 3), (8, 0, 4), (8, 0, 5), (8, 0, 6), (8, 0, 7), (8, 0, 8), (8, 0, 9), (8, 0, 10), (8, 1, 0), (8, 1, 1), (8, 1, 2), (8, 1, 3), (8, 1, 4), (8, 1, 5), (8, 1, 6), (8, 1, 7), (8, 1, 8), (8, 1, 9), (8, 1, 10), (8, 2, 0), (8, 2, 1), (8, 2, 2), (8, 2, 3), (8, 2, 4), (8, 2, 5), (8, 2, 6), (8, 2, 7), (8, 2, 8), (8, 2, 9), (8, 2, 10), (8, 3, 0), (8, 3, 1), (8, 3, 2), (8, 3, 3), (8, 3, 4), (8, 3, 5), (8, 3, 6), (8, 3, 7), (8, 3, 8), (8, 3, 9), (8, 3, 10), (8, 4, 0), (8, 4, 1), (8, 4, 2), (8, 4, 3), (8, 4, 4), (8, 4, 5), (8, 4, 6), (8, 4, 7), (8, 4, 8), (8, 4, 9), (8, 4, 10), (8, 5, 0), (8, 5, 1), (8, 5, 2), (8, 5, 3), (8, 5, 4), (8, 5, 5), (8, 5, 6), (8, 5, 7), (8, 5, 8), (8, 5, 9), (8, 5, 10), (8, 6, 0), (8, 6, 1), (8, 6, 2), (8, 6, 3), (8, 6, 4), (8, 6, 5), (8, 6, 6), (8, 6, 7), (8, 6, 8), (8, 6, 9), (8, 6, 10), (8, 7, 0), (8, 7, 1), (8, 7, 2), (8, 7, 3), (8, 7, 4), (8, 7, 5), (8, 7, 6), (8, 7, 7), (8, 7, 8), (8, 7, 9), (8, 7, 10), (8, 8, 0), (8, 8, 1), (8, 8, 2), (8, 8, 3), (8, 8, 4), (8, 8, 5), (8, 8, 6), (8, 8, 7), (8, 8, 8), (8, 8, 9), (8, 8, 10), (8, 9, 0), (8, 9, 1), (8, 9, 2), (8, 9, 3), (8, 9, 4), (8, 9, 5), (8, 9, 6), (8, 9, 7), (8, 9, 8), (8, 9, 9), (8, 9, 10), (8, 10, 0), (8, 10, 1), (8, 10, 2), (8, 10, 3), (8, 10, 4), (8, 10, 5), (8, 10, 6), (8, 10, 7), (8, 10, 8), (8, 10, 9), (8, 10, 10), (9, 0, 0), (9, 0, 1), (9, 0, 2), (9, 0, 3), (9, 0, 4), (9, 0, 5), (9, 0, 6), (9, 0, 7), (9, 0, 8), (9, 0, 9), (9, 0, 10), (9, 1, 0), (9, 1, 1), (9, 1, 2), (9, 1, 3), (9, 1, 4), (9, 1, 5), (9, 1, 6), (9, 1, 7), (9, 1, 8), (9, 1, 9), (9, 1, 10), (9, 2, 0), (9, 2, 1), (9, 2, 2), (9, 2, 3), (9, 2, 4), (9, 2, 5), (9, 2, 6), (9, 2, 7), (9, 2, 8), (9, 2, 9), (9, 2, 10), (9, 3, 0), (9, 3, 1), (9, 3, 2), (9, 3, 3), (9, 3, 4), (9, 3, 5), (9, 3, 6), (9, 3, 7), (9, 3, 8), (9, 3, 9), (9, 3, 10), (9, 4, 0), (9, 4, 1), (9, 4, 2), (9, 4, 3), (9, 4, 4), (9, 4, 5), (9, 4, 6), (9, 4, 7), (9, 4, 8), (9, 4, 9), (9, 4, 10), (9, 5, 0), (9, 5, 1), (9, 5, 2), (9, 5, 3), (9, 5, 4), (9, 5, 5), (9, 5, 6), (9, 5, 7), (9, 5, 8), (9, 5, 9), (9, 5, 10), (9, 6, 0), (9, 6, 1), (9, 6, 2), (9, 6, 3), (9, 6, 4), (9, 6, 5), (9, 6, 6), (9, 6, 7), (9, 6, 8), (9, 6, 9), (9, 6, 10), (9, 7, 0), (9, 7, 1), (9, 7, 2), (9, 7, 3), (9, 7, 4), (9, 7, 5), (9, 7, 6), (9, 7, 7), (9, 7, 8), (9, 7, 9), (9, 7, 10), (9, 8, 0), (9, 8, 1), (9, 8, 2), (9, 8, 3), (9, 8, 4), (9, 8, 5), (9, 8, 6), (9, 8, 7), (9, 8, 8), (9, 8, 9), (9, 8, 10), (9, 9, 0), (9, 9, 1), (9, 9, 2), (9, 9, 3), (9, 9, 4), (9, 9, 5), (9, 9, 6), (9, 9, 7), (9, 9, 8), (9, 9, 9), (9, 9, 10), (9, 10, 0), (9, 10, 1), (9, 10, 2), (9, 10, 3), (9, 10, 4), (9, 10, 5), (9, 10, 6), (9, 10, 7), (9, 10, 8), (9, 10, 9), (9, 10, 10), (10, 0, 0), (10, 0, 1), (10, 0, 2), (10, 0, 3), (10, 0, 4), (10, 0, 5), (10, 0, 6), (10, 0, 7), (10, 0, 8), (10, 0, 9), (10, 0, 10), (10, 1, 0), (10, 1, 1), (10, 1, 2), (10, 1, 3), (10, 1, 4), (10, 1, 5), (10, 1, 6), (10, 1, 7), (10, 1, 8), (10, 1, 9), (10, 1, 10), (10, 2, 0), (10, 2, 1), (10, 2, 2), (10, 2, 3), (10, 2, 4), (10, 2, 5), (10, 2, 6), (10, 2, 7), (10, 2, 8), (10, 2, 9), (10, 2, 10), (10, 3, 0), (10, 3, 1), (10, 3, 2), (10, 3, 3), (10, 3, 4), (10, 3, 5), (10, 3, 6), (10, 3, 7), (10, 3, 8), (10, 3, 9), (10, 3, 10), (10, 4, 0), (10, 4, 1), (10, 4, 2), (10, 4, 3), (10, 4, 4), (10, 4, 5), (10, 4, 6), (10, 4, 7), (10, 4, 8), (10, 4, 9), (10, 4, 10), (10, 5, 0), (10, 5, 1), (10, 5, 2), (10, 5, 3), (10, 5, 4), (10, 5, 5), (10, 5, 6), (10, 5, 7), (10, 5, 8), (10, 5, 9), (10, 5, 10), (10, 6, 0), (10, 6, 1), (10, 6, 2), (10, 6, 3), (10, 6, 4), (10, 6, 5), (10, 6, 6), (10, 6, 7), (10, 6, 8), (10, 6, 9), (10, 6, 10), (10, 7, 0), (10, 7, 1), (10, 7, 2), (10, 7, 3), (10, 7, 4), (10, 7, 5), (10, 7, 6), (10, 7, 7), (10, 7, 8), (10, 7, 9), (10, 7, 10), (10, 8, 0), (10, 8, 1), (10, 8, 2), (10, 8, 3), (10, 8, 4), (10, 8, 5), (10, 8, 6), (10, 8, 7), (10, 8, 8), (10, 8, 9), (10, 8, 10), (10, 9, 0), (10, 9, 1), (10, 9, 2), (10, 9, 3), (10, 9, 4), (10, 9, 5), (10, 9, 6), (10, 9, 7), (10, 9, 8), (10, 9, 9), (10, 9, 10), (10, 10, 0), (10, 10, 1), (10, 10, 2), (10, 10, 3), (10, 10, 4), (10, 10, 5), (10, 10, 6), (10, 10, 7), (10, 10, 8), (10, 10, 9), (10, 10, 10)]\n",
      "Therminal state rewards: {(0, 0, 0): -1.0, (0, 1, 0): -1.0, (0, 2, 0): -1.0, (0, 3, 0): -1.0, (0, 4, 0): -1.0, (0, 5, 0): -1.0, (0, 6, 0): -1.0, (0, 7, 0): -1.0, (0, 8, 0): -1.0, (0, 9, 0): -1.0, (0, 10, 0): -1.0, (1, 0, 0): -1.0, (1, 1, 0): -1.0, (1, 2, 0): -1.0, (1, 3, 0): -1.0, (1, 4, 0): -1.0, (1, 5, 0): -1.0, (1, 6, 0): -1.0, (1, 7, 0): -1.0, (1, 8, 0): -1.0, (1, 9, 0): -1.0, (1, 10, 0): -1.0, (2, 0, 0): -1.0, (2, 1, 0): -1.0, (2, 2, 0): -1.0, (2, 3, 0): -1.0, (2, 4, 0): -1.0, (2, 5, 0): -1.0, (2, 6, 0): -1.0, (2, 7, 0): -1.0, (2, 8, 0): -1.0, (2, 9, 0): -1.0, (2, 10, 0): -1.0, (3, 0, 0): -1.0, (3, 1, 0): -1.0, (3, 2, 0): -1.0, (3, 3, 0): -1.0, (3, 4, 0): -1.0, (3, 5, 0): -1.0, (3, 6, 0): -1.0, (3, 7, 0): -1.0, (3, 8, 0): -1.0, (3, 9, 0): -1.0, (3, 10, 0): -1.0, (4, 0, 0): -1.0, (4, 1, 0): -1.0, (4, 2, 0): -1.0, (4, 3, 0): -1.0, (4, 4, 0): -1.0, (4, 5, 0): -1.0, (4, 6, 0): -1.0, (4, 7, 0): -1.0, (4, 8, 0): -1.0, (4, 9, 0): -1.0, (4, 10, 0): -1.0, (5, 0, 0): -1.0, (5, 1, 0): -1.0, (5, 2, 0): -1.0, (5, 3, 0): -1.0, (5, 4, 0): -1.0, (5, 5, 0): 1.0, (5, 6, 0): -1.0, (5, 7, 0): -1.0, (5, 8, 0): -1.0, (5, 9, 0): -1.0, (5, 10, 0): -1.0, (6, 0, 0): -1.0, (6, 1, 0): -1.0, (6, 2, 0): -1.0, (6, 3, 0): -1.0, (6, 4, 0): -1.0, (6, 5, 0): -1.0, (6, 6, 0): -1.0, (6, 7, 0): -1.0, (6, 8, 0): -1.0, (6, 9, 0): -1.0, (6, 10, 0): -1.0, (7, 0, 0): -1.0, (7, 1, 0): -1.0, (7, 2, 0): -1.0, (7, 3, 0): -1.0, (7, 4, 0): -1.0, (7, 5, 0): -1.0, (7, 6, 0): -1.0, (7, 7, 0): -1.0, (7, 8, 0): -1.0, (7, 9, 0): -1.0, (7, 10, 0): -1.0, (8, 0, 0): -1.0, (8, 1, 0): -1.0, (8, 2, 0): -1.0, (8, 3, 0): -1.0, (8, 4, 0): -1.0, (8, 5, 0): -1.0, (8, 6, 0): -1.0, (8, 7, 0): -1.0, (8, 8, 0): -1.0, (8, 9, 0): -1.0, (8, 10, 0): -1.0, (9, 0, 0): -1.0, (9, 1, 0): -1.0, (9, 2, 0): -1.0, (9, 3, 0): -1.0, (9, 4, 0): -1.0, (9, 5, 0): -1.0, (9, 6, 0): -1.0, (9, 7, 0): -1.0, (9, 8, 0): -1.0, (9, 9, 0): -1.0, (9, 10, 0): -1.0, (10, 0, 0): -1.0, (10, 1, 0): -1.0, (10, 2, 0): -1.0, (10, 3, 0): -1.0, (10, 4, 0): -1.0, (10, 5, 0): -1.0, (10, 6, 0): -1.0, (10, 7, 0): -1.0, (10, 8, 0): -1.0, (10, 9, 0): -1.0, (10, 10, 0): -1.0}\n"
     ]
    }
   ],
   "source": [
    "# Solution:\n",
    "\n",
    "# State from l = 10\n",
    "staa = []\n",
    "x=0\n",
    "y=0\n",
    "z=0\n",
    "\n",
    "for x in range(11):\n",
    "    for y in range(11):\n",
    "        for z in range(11):\n",
    "            staa.append((x, y, z))\n",
    "\n",
    "result = list(\n",
    "    filter(\n",
    "        lambda tup: tup[2] == 0,\n",
    "        staa\n",
    "    )\n",
    ")\n",
    "\n",
    "# rewards and punishment for terminal state spaces \n",
    "my_dict = {}\n",
    "for i in result:\n",
    "    if(i == (5,5,0)):\n",
    "        my_dict[i] = 1.0\n",
    "    else:\n",
    "        my_dict[i] = -1.0\n",
    "        \n",
    "        \n",
    "print(\"L=10\", staa)\n",
    "print(\"Therminal state rewards:\", my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part H\n",
    "\n",
    "**Question 1:** Why does the cumulative reward start off around -0.5 at the beginning of the training?\n",
    "\n",
    "**Question 2:** Why will it be difficult for us to train the drone to reliably obtain rewards much greater than about 0.8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 Answer:** `The comulative rewards always start off at -0.5 because of how the probability if broken down. Because of the use of implementing the greedy approach, we would have our two action of keeping the currently policy or changing it to the new maximum. Leaving it to be the 1-0.5 but because in our situation with the descrete movmenet it would be a negative 0.5. `"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 Answer:** `Besides the fact of the million of runs the drone would have to preform to get a number close to 0.8, its also because its almost impossible to get it that high. Not only will there be hundreds of other varibles like weather and outside recourses, the thought prosses of constantly updating the policy will cause the reward to be a punishment on a run that might of been the most optimal on that action.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
